{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and visualization with pandas\n",
    "\n",
    "This notebook will introduce you to intermediate tasks using the [`pandas`](https://pandas.pydata.org/) data analysis library and demonstrate how to dedupe, clean, group, merge/join and visualize a data set.\n",
    "\n",
    "The data for this exercise will be a CSV of opioid deaths reported in Minnesota. Many examples here are adapted from work by Mary Jo Webster.\n",
    "\n",
    "(If you're completely new to Python or your syntax is rusty, it might be useful to [keep this notebook open in a new tab](Python%20syntax%20cheat%20sheet.ipynb) as a reference.)\n",
    "\n",
    "#### Session outline\n",
    "- [Import pandas; load a csv into a data frame](#Import-pandas;-load-a-csv-into-a-data-frame)\n",
    "- [Inspect the data](#Inspect-the-data)\n",
    "- [Checking for duplicate records](#Checking-for-duplicate-records-with-groupby())\n",
    "- [Renaming a column](#Renaming-a-column)\n",
    "- [Recoding data with lambda()](#Recoding-data-with-lambda())\n",
    "- [Cleaning up inconsistent values](#Cleaning-up-inconsistent-values)\n",
    "- [Visualize the data with a bar chart](#Visualize-the-data-with-a-bar-chart)\n",
    "- [Extracting year from a date](#Extracting-year-from-a-date)\n",
    "- [Merging with other data (Joins)](#Merging-with-other-data)\n",
    "- [Mapping TK](#Mapping-TK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For reference: Using Jupyter notebooks\n",
    "\n",
    "There are many ways to write and run Python code on your computer. One way -- the method we're using today -- is to use [Jupyter notebooks](https://jupyter.org/), which run in your browser and allow you to intersperse documentation with your code. They're handy for bundling your code with a human-readable explanation of what's happening at each step. Check out some examples from the [L.A. Times](https://github.com/datadesk/notebooks) and [BuzzFeed News](https://github.com/BuzzFeedNews/everything#data-and-analyses).\n",
    "\n",
    "**To add a new cell to your notebook**: Click the + button in the menu.\n",
    "\n",
    "**To run a cell of code**: Select the cell and click the \"Run\" button in the menu, or you can press Shift+Enter.\n",
    "\n",
    "**One common gotcha**: The notebook doesn't \"know\" about code you've written until you've _run_ the cell containing it. For example, if you define a variable called `my_name` in one cell, and later, when you try to access that variable in another cell but get an error that says `NameError: name 'my_name' is not defined`, the most likely solution is to run (or re-run) the cell in which you defined `my_name`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pandas; load a csv into a data frame\n",
    "\n",
    "This was covered in more detail in the previous session, so we'll keep moving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/opiate_deaths.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data\n",
    "\n",
    "Let's quickly check what columns are in our data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and see how many rows and columns are in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning your data\n",
    "\n",
    "When you receive data, it will almost always need some cleanup to be ready for analysis and/or display. There are some common issues to look for when you have acquired a new dataset.\n",
    "\n",
    "- Duplicate records\n",
    "- Misspellings\n",
    "- Inconsistent category/variable names\n",
    "- Null or empty (`''`) values that may affect calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for duplicate records with groupby()\n",
    "\n",
    "Each record has a `STATEID`, which seems to imply that each row represents a unique death. But how do we know? We can use the `groupby()` method to count how many times each `STATEID` value appears. If each row is a unique person or case, there should be only 1 copy of each value.\n",
    "\n",
    "In pandas we can chain several methods together to achieve our desired output.\n",
    "\n",
    "Here's the full command (we'll break down the details next):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['STATEID', 'DEATHCITY']].groupby('STATEID').count().reset_index().sort_values('DEATHCITY', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby, one piece at a time\n",
    "\n",
    "First, let's run our groupby on a subset of the data:\n",
    "\n",
    "`df[['STATEID', 'DEATHCITY']]`\n",
    "\n",
    "This will return a view of the dataframe that only includes the STATEID and DEATHCITY columns.\n",
    "\n",
    "Why do we even need `DEATHCITY` in this case? Because `groupby()` calls need at least two fields to work right. One or more fields that we are grouping on (`STATEID` in this case), and another field to be counted.\n",
    "\n",
    "`df[['STATEID', 'DEATHCITY']].groupby('STATEID')`\n",
    "\n",
    "And we'll add a `count()` call to tell pandas that we want to count the number of rows in each group, as opposed to other methods like summing or finding the average of that group.\n",
    "\n",
    "`df[['STATEID', 'DEATHCITY']].groupby('STATEID').count()`\n",
    "\n",
    "Next we need to include a weird pandas thing, `reset_index()` This is because otherwise, when performing a groupby, pandas will produce a MultiIndex, which is more complex than we need.\n",
    "\n",
    "`df[['STATEID', 'DEATHCITY']].groupby('STATEID').count().reset_index()`\n",
    "\n",
    "And finally we'll sort the results by our counted column, `DEATHCITY`. We want to surface any non-unique `STATEID`s, so we'll sort in descending order.\n",
    "\n",
    "`df[['STATEID', 'DEATHCITY']].groupby('STATEID').count().reset_index().sort_values('DEATHCITY', ascending=False)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great news, the highest count is 1, meaning there are no `STATEID`s that occur more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Your turn\n",
    "\n",
    "Let's find out how many deaths occured in each city. Take the next few minutes to see if you can group the datafame by the `DEATHCITY` column and count the number of rows from each city.\n",
    "\n",
    "Here's a start for you to build on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_by_city = df[['DEATHCITY', 'STATEID']]  # Your code herey..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove before class\n",
    "deaths_by_city = df[['DEATHCITY', 'STATEID']].groupby('DEATHCITY').count().reset_index().sort_values('STATEID', ascending=False)\n",
    "deaths_by_city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming a column\n",
    "\n",
    "It's confusing that the count of cities is held in a column called `STATEID`, which doesn't really have any meaning. It's always a good idea to have column headers that you can clearly understand, so let's rename the `STATEID` column in our `groupby()` to `death_count`. Note that this does NOT change the `STATEID` column in the original dataframe, as we are now working on a copy of a subset of that original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_by_city.rename(columns={'STATEID': 'death_count'}, inplace=True)\n",
    "deaths_by_city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note on `inplace=True`:** \n",
    "\n",
    "Most functions that you run on your dataframes need to be assigned to variables in order to be accessed at a later time. For example, if we were to write...\n",
    "\n",
    "`df['DEATHCITY'].drop_duplicates()`\n",
    "\n",
    "... to get a listing of unique cities in which people died, that would not affect our dataframe `df`. If we wanted it to store that new data somewhere to access later, we would would have to write:\n",
    "\n",
    "`unique_cities = df['DEATHCITY'].drop_duplicates()`\n",
    "\n",
    "We could also add `inplace=True`  the function without assigning it to a variable and the dataframe will be changed:\n",
    "\n",
    "`df['DEATHCITY'].drop_duplicates(inplace=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoding data with lambda()\n",
    "\n",
    "We can also use pandas to change the data values to something more presentable or consistent. For example, let's look at the unique values for the `GENDER` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.GENDER.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change all the values in `GENDER` to the slightly more user-friendly `Male` and `Female`. We'll use a lambda function. A lambda function is shorthand way of writing what you would like to do to each row of data in this column. It's usually safer to add a new column than to change the underlying data, so we'll create a new column called `gender_clean`, and populate it with values from the `GENDER` column that we have run through our little lambda factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender_clean'] = df['GENDER'].apply(lambda x: 'Male' if x == 'M' else 'Female')\n",
    "df['gender_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, each row was either labeled M or F, but our basic `if this, else that` logic is too naive for most real-world situations. For example, what if some rows had a value of `NA` or were null? In the above function, those rows would have gotten erroneously marked as `Female`. That's correction waiting to happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A slightly more complex `lambda()`\n",
    "\n",
    "To introduce more complex logic, we'll still use a lambda(). But instead of cramming the logic of what we want to do into one line, we'll use the lambda to apply an external function to each row.\n",
    "\n",
    "Here's the function we'll apply to each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_gender(input):\n",
    "    if input == 'M':\n",
    "        return 'Male'\n",
    "    elif input == 'F':\n",
    "        return 'Female'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may not have thought of every possible exception, but at least we won't wrongly assign some values to either male or female.\n",
    "\n",
    "Now let's apply the `decode_gender` function to each row's `GENDER` column using lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender_clean'] = df['GENDER'].apply(lambda x: decode_gender(x))\n",
    "df['gender_clean'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up inconsistent values\n",
    "\n",
    "Now let's take a look at the values in the `HISPANICETHNICITY` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.HISPANICETHNICITY.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, kind of a mess! We can't easily groupby these values right now because of the inconsistent capitalization and spelling.\n",
    "\n",
    "One way to harmonize data in a column is to set all of the letters to uppercase. We'll start a new `hispanic_clean` column to avoid introducing errors into our original `HISPANICETHNICITY` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hispanic_clean'] = df['HISPANICETHNICITY'].str.upper()\n",
    "df['hispanic_clean'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we cut our 8 unique values down to 5. But we still have inconsistencies in the spelling and hyphenation.\n",
    "\n",
    "Let's deal with the simple hyphens first, by searching for and replacing instances of `NON HISPANIC`, sans hyphen. You'll notice that now we're modifying `hispanic_clean`, which we just created, directly. If we ran this against the original `HISPANICETHNICITY` column, we'd mess up some of our previous work on `hispanic_clean`.\n",
    "\n",
    "Like most search and replace functions, the arguments passed to `replace()` are writtin the order of `1. Needle` in the `2. Haystack` you are searching through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hispanic_clean'] = df['hispanic_clean'].str.replace('NON HISPANIC', 'NON-HISPANIC')\n",
    "df['hispanic_clean'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better again! Now let's tackle the `NOT HISPANIC` rows, some of which have a hyphen and some that don't. We could write two lines that are similar to what we just did, but let's throw in a regular expression that can account for cases both with and without the hyphen.\n",
    "\n",
    "`.replace(r'NOT(-| )HISPANIC', 'NON-HISPANIC')`\n",
    "\n",
    "The `r` preceding the string tells python this is a regular expression, not a normal string.\n",
    "\n",
    "The `(-| )` means that either a hypen or a space is an acceptable value to call a match. In either case, the command will write `NON-HISPANIC` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hispanic_clean'] = df['hispanic_clean'].str.replace(r'NOT(-| )HISPANIC', 'NON-HISPANIC')\n",
    "df['hispanic_clean'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a tidy column we have now!\n",
    "\n",
    "### ✍️ Your turn\n",
    "\n",
    "In the cell below, group by and count the `df` data frame:\n",
    "- Get a subset of the `df` data frame\n",
    "- Group by `hispanic_clean`\n",
    "- Count the rows with each value\n",
    "- Order descending\n",
    "- Rename the count to a new variable called `death_count` after grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deaths_by_hispanic_ethnicity = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove before class\n",
    "deaths_by_hispanic_ethnicity = df[['hispanic_clean', 'STATEID']].groupby('hispanic_clean').count().reset_index().sort_values('STATEID', ascending=False)\n",
    "deaths_by_hispanic_ethnicity.rename(columns={'STATEID': 'death_count'}, inplace=True)\n",
    "deaths_by_hispanic_ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data, the vast majority of deaths are from people without Hispanic ethnicity. But Minnesota is a pretty white state, so we need to look at these numbers as a percentage, so we can compare the deaths to Minnesota's overall Hispanic population, which is about 5% of the state's total population, according to Pew Research.\n",
    "\n",
    "We can create a new column to calculate the percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_by_hispanic_ethnicity['pct_total'] = deaths_by_hispanic_ethnicity['death_count'] / deaths_by_hispanic_ethnicity['death_count'].sum()\n",
    "deaths_by_hispanic_ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data with a bar chart\n",
    "\n",
    "Pandas works well with a visualization package called [`matplotlib`](https://matplotlib.org/). There are several other visualization packages that have robust feature sets, including [Altair](https://altair-viz.github.io/), but matplotlib is already tightly integrated with pandas, so we'll start with that.\n",
    "\n",
    "It doesn't make for the most compelling chart in the world, but once you have your data cleaned up, it's easy to make a simple bar chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_by_hispanic_ethnicity.plot.bar(x='hispanic_clean', y='death_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting year from a date\n",
    "\n",
    "To look at the trend in deaths over time, we can grab just the year from the `DEATHDATE` column by taking advantage of Python's date-handling abilities. First we need to tell pandas that the `DEATHDATE` column is a `datetime`, not just an ordinary string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DEATHDATE'] = pd.to_datetime(df['DEATHDATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `lambda` to call `.year` on each `DEATHDATE`, and pipe it to a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['death_year'] = df['DEATHDATE'].apply(lambda x: x.year)\n",
    "df['death_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Your turn\n",
    "\n",
    "In the cells below, group by and count the `df` data frame:\n",
    "- Get a subset of the `df` data frame\n",
    "- Group by `death_year`\n",
    "- Count the rows with each value\n",
    "- Order by death year\n",
    "- Rename the count to a new variable called `death_count` after grouping\n",
    "- Plot the death years on a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deaths_by_year ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove before class\n",
    "deaths_by_year = df[['death_year', 'STATEID']].groupby('death_year').count().reset_index().sort_values('death_year')\n",
    "deaths_by_year.rename(columns={'STATEID': 'death_count'}, inplace=True)\n",
    "deaths_by_year.plot.bar(x='death_year', y='death_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging with other data\n",
    "\n",
    "If you remember our city `deaths_by_city` data from above, it's not surprising that Minnesota's largest city has the highest raw death count -- there are so many more people. The data would be a lot more meaningful if we could calculate the deaths per capita. But population isn't in our original CSV. But we can join our data with other data sources, provided they have some kind of identical value to join on.\n",
    "\n",
    "First, we load a spreadsheet of population data from a reliable source from a similar time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_estimates = pd.read_csv('data/mn_cities_pop_estimates.csv')\n",
    "pop_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh, the cities in this data set are Title Case, and the cities in our opiates data are ALL UPPERCASE.\n",
    "\n",
    "Let's make the population csv's city names all uppercase as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_estimates['city_name_capped'] = pop_estimates['City Name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_by_city_merged = deaths_by_city.merge(\n",
    "    pop_estimates,\n",
    "    how=\"left\",\n",
    "    left_on=\"DEATHCITY\",\n",
    "    right_on=\"city_name_capped\"\n",
    ")\n",
    "deaths_by_city_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Your turn\n",
    "\n",
    "In the cells below:\n",
    "- Create a `deaths_per_capita` field for each city by dividing `death_count` by `Total Population, 2018`\n",
    "- Order the data frame by `deaths_per_capita`\n",
    "- Plot the per capita deaths on a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deaths_by_city_merged['deaths_per_capita'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove before class\n",
    "deaths_by_city_merged['deaths_per_capita'] = deaths_by_city_merged['death_count'] / deaths_by_city_merged['Total Population, 2018']\n",
    "deaths_by_city_merged.sort_values('deaths_per_capita', ascending=False, inplace=True)\n",
    "deaths_by_city_merged['deaths_per_capita']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, that is just too many cities, and when we're doing per capita we need to be cautious about small denominators. Let's restrict the chart to only cities with 30 or more deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_by_city_30plus = deaths_by_city_merged[deaths_by_city_merged['death_count'] > 30]\n",
    "deaths_by_city_30plus.plot.bar(x='DEATHCITY', y='deaths_per_capita')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's great to see these death rates on a bar chart, but a map would help us visualize this geographic dataset in another way.\n",
    "\n",
    "First, there are a few additional libraries we need to bring in to make this all work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import mapclassify\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of city, we're going to work with counties. \n",
    "\n",
    "Now we want to bring in our shapefile of Minnesota counties. A shapefile is a mapping file. In this case, the Minnesota counties shapefile defines the boundary lines between different counties in the state of Minnesota. You can find your [shapefiles for your state here](https://www.census.gov/cgi-bin/geo/shapefiles/index.php)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_cnty = gpd.read_file('gis/tl_2015_mn_county/tl_2015_mn_county.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this looks different than how we've brought data in before. That's because plain jane pandas can't read these types of files. We need to use an additional library called geopandas.\n",
    "\n",
    "But once you import it you can query it like any other dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mn_cnty.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to join this shapefile with our death data so we can start to visualize where these deaths are happeneing. In order to do that, we need to find a common column to merge on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mn_cnty.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know (because I added it myself) that the `GEOID` column from my `mn_cnty` shapefile is the same as the `INJURY_FIPS` column from my `df` data file. So, in order to merge these two together, I'm going to have to group by deaths data by county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_by_cnty = df[['INJURY_FIPS', 'STATEID']].groupby('INJURY_FIPS').count().reset_index().sort_values('STATEID', ascending=False)\n",
    "deaths_by_cnty['INJURY_FIPS'] = deaths_by_cnty['INJURY_FIPS'].astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That second line converts the data type of our `INJURY_FIPS` field to a string. This is important because the `GEOID` field in our shapefile is stored as a string and we cannot merge two dataframes together on columns with mismatched data types.\n",
    "\n",
    "I know that might sound like I'm speaking a different language but just tuck that knowledge away somewhere and some day you'll get an error message `ValueError: You are trying to merge on object and float64 columns.` and you'll be like... What did that woman tell me about merging on mismatched data types..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deaths_by_cnty_merged = mn_cnty.merge(\n",
    "    deaths_by_cnty,\n",
    "    how=\"left\",\n",
    "    left_on=\"GEOID\",\n",
    "    right_on=\"INJURY_FIPS\"\n",
    ")\n",
    "deaths_by_cnty_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok look at that! Merged!\n",
    "\n",
    "Now we have to clean up our data just a touch before we can map this puppy.\n",
    "\n",
    "First, we're going to rename that `STATEID` column because, again, it's actually our death counts column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_by_cnty_merged.rename(columns={'STATEID': 'death_count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we're going to replace all of the `NA` values in that death_counts column with 0 values. Because we joined the death data INTO the shapefile, it is possible that we will have counties without deaths.\n",
    "\n",
    "And that is the case. We have 87 counties in Minnesota:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mn_cnty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And only 77 of those counties have death data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(deaths_by_cnty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the change. Here are all the values found in the `death_count` field in our merged dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_by_cnty_merged['death_count'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our `fillna` function and look again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deaths_by_cnty_merged.fillna(value={'death_count': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deaths_by_cnty_merged['death_count'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look! No more `nan` values! This will allow us to visualize counties with 0 deaths on the map. If we left those as `nan` valuse, the map would omit the county all together and we'd have a swiss cheese map on our hands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fix our map projection so it looks prettier\n",
    "deaths_by_cnty_merged = deaths_by_cnty_merged.to_crs('EPSG:3395')\n",
    "\n",
    "#establish the max and min values for our legend\n",
    "vmin = deaths_by_cnty_merged.death_count.min()\n",
    "vmax = deaths_by_cnty_merged.death_count.max()\n",
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "# we don't need axes for a map\n",
    "ax.axis('off')\n",
    "\n",
    "# add a title\n",
    "ax.set_title('Minnesota Deaths by County', fontdict={'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "# create an annotation for the data source\n",
    "ax.annotate('Source: xxxxxx', xy=(0.1, .08),  xycoords='figure fraction', \n",
    "            horizontalalignment='left', verticalalignment='top', fontsize=12, color='#555555')\n",
    "\n",
    "#This bit will show us how many counties are in each bucket\n",
    "print(mapclassify.JenksCaspall(deaths_by_cnty_merged.death_count, k=5))\n",
    "\n",
    "#This bit actually builds the map\n",
    "deaths_by_cnty_merged.plot(column='death_count', cmap='Greens', linewidth=0.8, ax=ax, \n",
    "                           edgecolor='0.8', scheme='JenksCaspall', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoory we've made a map! But this isn't very illuminating because we've just mapped the death counts instead of death rates.\n",
    "\n",
    "### ✍️Try your hand at creating a map using death rates for each county by:\n",
    "\n",
    "- add county population data (look in the data folder)\n",
    "- create a new dataframe by merging our death by county data to the county pop data\n",
    "- calculate a new death rate column in your new dataframe\n",
    "- use the death rate column to create a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add county pop data\n",
    "cnty_pop = pd.read_csv('data/mn_cnty_pop_estimates.csv')\n",
    "cnty_pop['cnty_fips'] = cnty_pop['GEO_ID'].apply(lambda x: x[-5:])\n",
    "\n",
    "# create a new dataframe\n",
    "cnty_death_pop_merge = deaths_by_cnty_merged.merge(cnty_pop, how='left',\n",
    "                                                   left_on='INJURY_FIPS',\n",
    "                                                   right_on='cnty_fips')\n",
    "\n",
    "# calculate death rate column per 1,000 residents\n",
    "cnty_death_pop_merge['death_rate'] = (cnty_death_pop_merge['death_count'] / cnty_death_pop_merge['total_pop'])*1000\n",
    "\n",
    "# replace those na values with 0s\n",
    "cnty_death_pop_merge.fillna(value={'death_rate': 0}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#establish the max and min values for our legend\n",
    "vmin = cnty_death_pop_merge.death_rate.min()\n",
    "vmax = cnty_death_pop_merge.death_rate.max()\n",
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "# we don't need axes for a map\n",
    "ax.axis('off')\n",
    "\n",
    "# add a title\n",
    "ax.set_title('Minnesota Death Rate by County', fontdict={'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "# create an annotation for the data source\n",
    "ax.annotate('Source: xxxxxx', xy=(0.1, .08),  xycoords='figure fraction', \n",
    "            horizontalalignment='left', verticalalignment='top', fontsize=12, color='#555555')\n",
    "\n",
    "#This bit will show us how many counties are in each bucket\n",
    "print(mapclassify.JenksCaspall(cnty_death_pop_merge.death_rate, k=5))\n",
    "\n",
    "#This bit actually builds the map\n",
    "cnty_death_pop_merge.plot(column='death_rate', cmap='Greens', linewidth=0.8, ax=ax, \n",
    "                          edgecolor='0.8', scheme='JenksCaspall', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save it as an image file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('img/minnesota-cnty-od-rates-map.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If we need another example but this is too much already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping values into new categories\n",
    "\n",
    "For a more informative bar chart, let's tag each row with a new `age_group` value. We can use `lambda()` and `apply()`, as we did above.\n",
    "\n",
    "First, our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age_group(age):\n",
    "    if age < 20:\n",
    "        return '0-19'\n",
    "    elif age >=20 and age < 35:\n",
    "        return '20-34'\n",
    "    elif age >= 35 and age < 50:\n",
    "        return '35-49'\n",
    "    elif age >= 50 and age < 70:\n",
    "        return '50-69'\n",
    "    elif age >= 70:\n",
    "        return '70 and over'\n",
    "    return 'Unknown'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's apply it to each row's `AGEYEARS` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_group'] = df.AGEYEARS.apply(lambda x: get_age_group(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And be sure to check that the results make sense..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['age_group', 'AGEYEARS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Your turn\n",
    "\n",
    "In the cells below, group by and count the `df` data frame:\n",
    "- Get a subset of the `df` data frame\n",
    "- Group by `age_group`\n",
    "- Count the rows with each value\n",
    "- Order by age group\n",
    "- Rename the count to a new variable called `death_count` after grouping\n",
    "- Plot the age groups on a bar chart"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
